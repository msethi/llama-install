{"data":{"edges":[],"nodes":[{"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import os\nimport json\nimport fitz  # PyMuPDF\nimport pandas as pd\n\nfrom langflow.custom import Component\nfrom langflow.io import StrInput, BoolInput, Output\n\nclass PDFDirectoryJSONExtractor(Component):\n    display_name = \"PDF Directory → JSON\"\n    description  = \"Extract tables from PDFs and output JSON records separated by '| |'\"\n    icon         = \"file-code\"\n    name         = \"PDFDirectoryJSONExtractor\"\n\n    inputs = [\n        StrInput(\n            name=\"path\",\n            display_name=\"Directory Path\",\n            info=\"Folder containing your PDFs\",\n            value=\"/app/chroma_data\",\n        ),\n        StrInput(\n            name=\"file_name_filter\",\n            display_name=\"Filename Filter\",\n            info=\"Only PDFs with this substring (case-insensitive)\",\n            value=\"Remittance\",\n        ),\n        BoolInput(\n            name=\"recursive\",\n            display_name=\"Recursive\",\n            info=\"Search subfolders if checked\",\n            value=True,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            name=\"json_output\",\n            display_name=\"JSON Output\",\n            method=\"run_extraction\",\n        ),\n        Output(\n            name=\"status\",\n            display_name=\"Status\",\n            method=\"get_status\",\n        ),\n    ]\n\n    def extract_rows(self, file_path):\n        \"\"\"Return list of rows (each a list of cell strings) from one PDF.\"\"\"\n        doc = fitz.open(file_path)\n        rows = []\n        for page in doc:\n            blocks = page.get_text(\"dict\")[\"blocks\"]\n            line_items = []\n            for b in blocks:\n                if b[\"type\"] != 0:\n                    continue\n                for line in b[\"lines\"]:\n                    y = round(line[\"bbox\"][1], 1)\n                    text = \" \".join(span[\"text\"].strip() for span in line[\"spans\"] if span[\"text\"].strip())\n                    if text:\n                        line_items.append((y, text.split()))\n            # group by Y-coordinate\n            grouped = {}\n            for y, cells in line_items:\n                grouped.setdefault(y, []).extend(cells)\n            for y in sorted(grouped):\n                rows.append(grouped[y])\n        return rows\n\n    def run_extraction(self) -> str:\n        root   = self.path\n        filt   = self.file_name_filter.strip().lower()\n        recurse= self.recursive\n\n        if not os.path.isdir(root):\n            self._status = f\"Directory not found: {root}\"\n            return \"\"\n\n        # collect PDFs\n        pdfs = []\n        for dp, dns, fns in os.walk(root):\n            if not recurse:\n                dns.clear()\n            for fn in fns:\n                if fn.lower().endswith(\".pdf\") and filt in fn.lower():\n                    pdfs.append(os.path.join(dp, fn))\n\n        if not pdfs:\n            self._status = f\"No PDFs matching '{filt}' in {root}\"\n            return \"\"\n\n        all_rows = []\n        for pdf in pdfs:\n            try:\n                rows = self.extract_rows(pdf)\n                all_rows.extend(rows)\n            except Exception as e:\n                self._status = f\"Error reading {pdf}: {e}\"\n                return \"\"\n\n        if not all_rows:\n            self._status = \"No data extracted from any PDF.\"\n            return \"\"\n            \n        hdr_idx = None\n        for i, row in enumerate(all_rows):\n            if any(\"invoice\" in cell.lower() for cell in row):\n                hdr_idx = i\n                break\n\n        if hdr_idx is None:\n            self._status = \"Table header not found in PDF text.\"\n            return \"\"\n\n        header = all_rows[hdr_idx]\n        raw_data = all_rows[hdr_idx+1:]\n\n        # 2) Normalize each row to exactly len(header) columns\n        n_cols = len(header)\n        data = []\n        for row in raw_data:\n            if len(row) >= n_cols:\n                data.append(row[:n_cols])\n            else:\n                data.append(row + [None] * (n_cols - len(row)))\n\n        # 3) Build DataFrame\n        df = pd.DataFrame(data, columns=header)\n\n        # 4) Convert to JSON records\n        records = df.to_dict(orient=\"records\")\n        json_strings = [json.dumps(rec) for rec in records]\n        output = \"| |\".join(json_strings)\n\n        self._status = f\"Extracted {len(records)} records from {len(pdfs)} PDFs\"\n        return output\n\n    def get_status(self) -> str:\n        return getattr(self, \"_status\", \"\")\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"file_name_filter":{"tool_mode":false,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"file_name_filter","value":"Remittance","display_name":"Filename Filter","advanced":false,"dynamic":false,"info":"Only PDFs with this substring (case-insensitive)","title_case":false,"type":"str","_input_type":"StrInput"},"path":{"tool_mode":false,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"path","value":"/app/chroma_data","display_name":"Directory Path","advanced":false,"dynamic":false,"info":"Folder containing your PDFs","title_case":false,"type":"str","_input_type":"StrInput"},"recursive":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"recursive","value":true,"display_name":"Recursive","advanced":false,"dynamic":false,"info":"Search subfolders if checked","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Extract tables from PDFs and output JSON records separated by '| |'","icon":"file-code","base_classes":["Text"],"display_name":"PDF Directory → JSON","documentation":"","minimized":false,"custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Text"],"selected":"Text","name":"json_output","hidden":null,"display_name":"JSON Output","method":"run_extraction","value":"__UNDEFINED__","cache":true,"required_inputs":null,"allows_loop":false,"options":null,"tool_mode":true},{"types":["Text"],"selected":"Text","name":"status","hidden":null,"display_name":"Status","method":"get_status","value":"__UNDEFINED__","cache":true,"required_inputs":null,"allows_loop":false,"options":null,"tool_mode":true}],"field_order":["path","file_name_filter","recursive"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.4.3","official":false},"showNode":true,"type":"PDFDirectoryJSONExtractor","id":"CustomComponent-YfmzZ"},"id":"CustomComponent-YfmzZ","position":{"x":0,"y":0},"type":"genericNode"}],"viewport":{"x":1,"y":1,"zoom":1}},"description":"Extract tables from PDFs and output JSON records separated by '| |'","name":"PDF Directory → JSON","id":"CustomComponent-YfmzZ","is_component":true,"last_tested_version":"1.4.3"}